{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dajare_model_v1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "xeAiqFGC8AUd",
        "g-6r_HKYQW1U",
        "vNSxRndcQeQt",
        "zmgZreHFsMuj"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/richardyy1188/Information_Extraction_from_Biographies/blob/master/dajare_model_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "xeAiqFGC8AUd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Colab related"
      ]
    },
    {
      "metadata": {
        "id": "Ue9ckEw_7-WX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Check uptime (max 12 hr)\n",
        "!cat /proc/uptime | awk '{print $1 /60 /60 \"hours \"}'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bXMnI2gscaNt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5b6b3efb-b746-4415-efcf-4d3d30a1f849"
      },
      "cell_type": "code",
      "source": [
        "# upgrade numpy to resolve error of loading npy file\n",
        "# !pip install --upgrade numpy"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7.91463hours \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fOirBWrSWRDe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# files.upload()\n",
        "# files.download()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lMmonYgn8X-R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import & Load"
      ]
    },
    {
      "metadata": {
        "id": "b7hG0VgJJ09y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle, time\n",
        "from google.colab import files\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input, LSTM, Reshape, Lambda\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nb_UzAAqMzuS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('X_v1.pickle', 'rb') as f: bundles_X = pickle.load(f) # (num_bundles, Tx (varing), word_voc_size), every samples in the same bundle have the same length\n",
        "with open('Y_v1.pickle', 'rb') as f: bundles_Y = pickle.load(f)\n",
        "with open('index2word.pickle', 'rb') as f: i2w = pickle.load(f)\n",
        "with open('word2index.pickle', 'rb') as f: w2i = pickle.load(f)\n",
        "word_voc_size = len(i2w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g-6r_HKYQW1U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter"
      ]
    },
    {
      "metadata": {
        "id": "jmtw3ifD32RE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "n_a = 128 # dimension of hidden states of LSTM\n",
        "learning_rate = 0.005\n",
        "learning_rate_decay = 0.01\n",
        "batch_size = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vNSxRndcQeQt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model Building"
      ]
    },
    {
      "metadata": {
        "id": "ezPsmY8RvBRp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lstm = LSTM(n_a, return_sequences=True, return_state=True) # default activation tanh\n",
        "densor = Dense(word_voc_size, activation='softmax')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xh0xrOL86rMm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model(n_a, word_voc_size):\n",
        "  inputs = Input(shape=(None, word_voc_size)) # None for varing (here, varing Tx)\n",
        "  X, _, _ = lstm(inputs) # return  (all step outputs, last step output, last step cell state)\n",
        "  outputs = densor(X)\n",
        "  return Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9ulxOVdyyEhB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "78569db7-4a9b-4689-9e9d-11d7713d1727"
      },
      "cell_type": "code",
      "source": [
        "model = model(n_a, word_voc_size)\n",
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, None, 45973)       0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  [(None, None, 128), (None 23604224  \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, None, 45973)       5930517   \n",
            "=================================================================\n",
            "Total params: 29,534,741\n",
            "Trainable params: 29,534,741\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XEusAizDygsr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "opt = Adam(lr=learning_rate, decay=learning_rate_decay)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j1ZiSLghrdF4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train Model"
      ]
    },
    {
      "metadata": {
        "id": "tQ_myP5LrnlZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## helper function"
      ]
    },
    {
      "metadata": {
        "id": "cyfDUo6nrqL0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def chunks(lst, size): # equally chunk the list\n",
        "  for i in range(0, len(lst), size):\n",
        "    yield lst[i:i + size]\n",
        "\n",
        "def to_one_hot_arr(indices, classes, first_zero=False):\n",
        "  arr = to_categorical(indices, classes)\n",
        "  if first_zero:\n",
        "    arr[0] = np.zeros((classes,))\n",
        "  return arr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MvTuJDKyTasV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train Process & Monitoring\n",
        "To take advantage of mini-batch gradient descent, under varying sequence length.  \n",
        "We feed by several times for different sequence length."
      ]
    },
    {
      "metadata": {
        "id": "6TN3Aqlm-Y36",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load_weights('model_v1-1_13_epochs.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tEv-Kft7zHlE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 12868
        },
        "outputId": "b2daddb6-02a1-4e1b-bdfa-b5116585138c"
      },
      "cell_type": "code",
      "source": [
        "true_epochs = 1\n",
        "losses = list()\n",
        "start_time = time.time()\n",
        "for i in range(true_epochs):\n",
        "  print('---------TRUE EPOCH {}/{}--------------'.format(i+1, true_epochs))\n",
        "  total_sample = 0\n",
        "  loss_weighted_sum = 0\n",
        "  for bundle_X, bundle_Y in zip(bundles_X, bundles_Y): # a bundle is samples with the same length\n",
        "    ch_size = 6000\n",
        "    # cut a bundle into small sub bundles, otherwise resulted np array will be too large for memory\n",
        "    for sub_bundle_X, sub_bundle_Y in zip(chunks(bundle_X,ch_size), chunks(bundle_Y,ch_size)):\n",
        "      \n",
        "      # preprocess: from indices to one-hots, and to np array which is only accepted input/output in Keras\n",
        "      sub_bundle_X =  np.array([to_one_hot_arr(sample, word_voc_size, True).astype('int8') for sample in sub_bundle_X]) \n",
        "      sub_bundle_Y =  np.array([to_one_hot_arr(sample, word_voc_size).astype('int8') for sample in sub_bundle_Y])\n",
        "      # train\n",
        "      history = model.fit(sub_bundle_X, sub_bundle_Y, batch_size=batch_size, verbose=0) # default epoch=1\n",
        "      # record loss\n",
        "      loss = history.history['loss']\n",
        "      loss_weighted_sum += loss * len(sub_bundle_X)\n",
        "      total_sample += len(sub_bundle_X)\n",
        "      \n",
        "  # end of a true epoch, calculate true loss and record\n",
        "  losses.append(loss_weighted_sum / total_sample)\n",
        "\n",
        "# Print info of training this time\n",
        "ms = time.time() - start_time\n",
        "minute = ms /60 % 60\n",
        "hr = ms /60 /60\n",
        "print(\"Spend time: {}hr {}mimutes\".format(hr, minute))\n",
        "for i, loss in enumerate(losses):\n",
        "  print(\"True epoch {}: {}\".format(i+1, loss))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------TRUE EPOCH 1/3--------------\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 36s 6ms/step - loss: 5.8148\n",
            "Epoch 1/1\n",
            "2378/2378 [==============================] - 15s 6ms/step - loss: 5.7353\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 29s 5ms/step - loss: 5.6346\n",
            "Epoch 1/1\n",
            "5463/5463 [==============================] - 27s 5ms/step - loss: 5.4092\n",
            "Epoch 1/1\n",
            "5960/5960 [==============================] - 40s 7ms/step - loss: 5.8062\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 33s 5ms/step - loss: 5.6302\n",
            "Epoch 1/1\n",
            "4502/4502 [==============================] - 25s 5ms/step - loss: 5.5691\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 22s 4ms/step - loss: 5.7046\n",
            "Epoch 1/1\n",
            "1073/1073 [==============================] - 4s 4ms/step - loss: 5.3463\n",
            "Epoch 1/1\n",
            "3834/3834 [==============================] - 28s 7ms/step - loss: 6.1511\n",
            "Epoch 1/1\n",
            "2420/2420 [==============================] - 19s 8ms/step - loss: 5.8691\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 25s 4ms/step - loss: 5.6217\n",
            "Epoch 1/1\n",
            "4244/4244 [==============================] - 18s 4ms/step - loss: 5.5410\n",
            "Epoch 1/1\n",
            "1805/1805 [==============================] - 5s 3ms/step - loss: 7.1252\n",
            "Epoch 1/1\n",
            "745/745 [==============================] - 7s 9ms/step - loss: 6.2504\n",
            "Epoch 1/1\n",
            "1401/1401 [==============================] - 12s 9ms/step - loss: 6.0641\n",
            "Epoch 1/1\n",
            "445/445 [==============================] - 4s 10ms/step - loss: 6.1179\n",
            "Epoch 1/1\n",
            "428/428 [==============================] - 1s 2ms/step - loss: 7.3628\n",
            "Epoch 1/1\n",
            "259/259 [==============================] - 3s 11ms/step - loss: 6.1977\n",
            "Epoch 1/1\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 6.5237\n",
            "Epoch 1/1\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 6.3875\n",
            "Epoch 1/1\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 6.1871\n",
            "Epoch 1/1\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 6.6607\n",
            "Epoch 1/1\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 6.2904\n",
            "Epoch 1/1\n",
            "13/13 [==============================] - 0s 34ms/step - loss: 6.4072\n",
            "Epoch 1/1\n",
            "45/45 [==============================] - 0s 2ms/step - loss: 8.0201\n",
            "Epoch 1/1\n",
            "6/6 [==============================] - 0s 66ms/step - loss: 6.6602\n",
            "Epoch 1/1\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 6.4844\n",
            "Epoch 1/1\n",
            "10/10 [==============================] - 0s 44ms/step - loss: 6.5847\n",
            "Epoch 1/1\n",
            "24/24 [==============================] - 0s 21ms/step - loss: 6.5169\n",
            "Epoch 1/1\n",
            "2/2 [==============================] - 0s 218ms/step - loss: 6.9602\n",
            "Epoch 1/1\n",
            "7/7 [==============================] - 0s 60ms/step - loss: 6.4635\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 1s 628ms/step - loss: 6.5874\n",
            "Epoch 1/1\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 6.2352\n",
            "Epoch 1/1\n",
            "2/2 [==============================] - 0s 225ms/step - loss: 6.2892\n",
            "Epoch 1/1\n",
            "2/2 [==============================] - 0s 212ms/step - loss: 5.7620\n",
            "---------TRUE EPOCH 2/3--------------\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 36s 6ms/step - loss: 5.7325\n",
            "Epoch 1/1\n",
            "2378/2378 [==============================] - 15s 6ms/step - loss: 5.6613\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 29s 5ms/step - loss: 5.6051\n",
            "Epoch 1/1\n",
            "5463/5463 [==============================] - 26s 5ms/step - loss: 5.3800\n",
            "Epoch 1/1\n",
            "5960/5960 [==============================] - 40s 7ms/step - loss: 5.7796\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 33s 5ms/step - loss: 5.5990\n",
            "Epoch 1/1\n",
            "4502/4502 [==============================] - 25s 5ms/step - loss: 5.5443\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 22s 4ms/step - loss: 5.6981\n",
            "Epoch 1/1\n",
            "1073/1073 [==============================] - 4s 4ms/step - loss: 5.3514\n",
            "Epoch 1/1\n",
            "3834/3834 [==============================] - 28s 7ms/step - loss: 6.0188\n",
            "Epoch 1/1\n",
            "2420/2420 [==============================] - 19s 8ms/step - loss: 5.8248\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 25s 4ms/step - loss: 5.5860\n",
            "Epoch 1/1\n",
            "4244/4244 [==============================] - 18s 4ms/step - loss: 5.5128\n",
            "Epoch 1/1\n",
            "1805/1805 [==============================] - 5s 3ms/step - loss: 7.0745\n",
            "Epoch 1/1\n",
            "745/745 [==============================] - 7s 9ms/step - loss: 6.1480\n",
            "Epoch 1/1\n",
            "1401/1401 [==============================] - 12s 9ms/step - loss: 5.9982\n",
            "Epoch 1/1\n",
            "445/445 [==============================] - 4s 10ms/step - loss: 6.0723\n",
            "Epoch 1/1\n",
            "428/428 [==============================] - 1s 2ms/step - loss: 7.3111\n",
            "Epoch 1/1\n",
            "259/259 [==============================] - 3s 11ms/step - loss: 6.1683\n",
            "Epoch 1/1\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 6.4988\n",
            "Epoch 1/1\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 6.3565\n",
            "Epoch 1/1\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 6.1607\n",
            "Epoch 1/1\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 6.6308\n",
            "Epoch 1/1\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 6.2678\n",
            "Epoch 1/1\n",
            "13/13 [==============================] - 0s 34ms/step - loss: 6.3832\n",
            "Epoch 1/1\n",
            "45/45 [==============================] - 0s 2ms/step - loss: 8.0215\n",
            "Epoch 1/1\n",
            "6/6 [==============================] - 0s 65ms/step - loss: 6.6376\n",
            "Epoch 1/1\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 6.4622\n",
            "Epoch 1/1\n",
            "10/10 [==============================] - 0s 43ms/step - loss: 6.5668\n",
            "Epoch 1/1\n",
            "24/24 [==============================] - 0s 21ms/step - loss: 6.5033\n",
            "Epoch 1/1\n",
            "2/2 [==============================] - 0s 218ms/step - loss: 6.9342\n",
            "Epoch 1/1\n",
            "7/7 [==============================] - 0s 60ms/step - loss: 6.4560\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 1s 629ms/step - loss: 6.5384\n",
            "Epoch 1/1\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 6.2047\n",
            "Epoch 1/1\n",
            "2/2 [==============================] - 0s 224ms/step - loss: 6.2649\n",
            "Epoch 1/1\n",
            "2/2 [==============================] - 0s 211ms/step - loss: 5.7603\n",
            "---------TRUE EPOCH 3/3--------------\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 36s 6ms/step - loss: 5.6942\n",
            "Epoch 1/1\n",
            "2378/2378 [==============================] - 15s 6ms/step - loss: 5.6154\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 29s 5ms/step - loss: 5.5761\n",
            "Epoch 1/1\n",
            "5463/5463 [==============================] - 27s 5ms/step - loss: 5.3657\n",
            "Epoch 1/1\n",
            "5960/5960 [==============================] - 40s 7ms/step - loss: 5.7341\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 33s 5ms/step - loss: 5.5649\n",
            "Epoch 1/1\n",
            "4502/4502 [==============================] - 25s 5ms/step - loss: 5.5161\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 21s 4ms/step - loss: 5.6791\n",
            "Epoch 1/1\n",
            "1073/1073 [==============================] - 4s 4ms/step - loss: 5.3583\n",
            "Epoch 1/1\n",
            "3834/3834 [==============================] - 28s 7ms/step - loss: 5.9464\n",
            "Epoch 1/1\n",
            "2420/2420 [==============================] - 19s 8ms/step - loss: 5.7797\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 25s 4ms/step - loss: 5.5681\n",
            "Epoch 1/1\n",
            "4244/4244 [==============================] - 18s 4ms/step - loss: 5.4900\n",
            "Epoch 1/1\n",
            "1805/1805 [==============================] - 5s 3ms/step - loss: 7.0430\n",
            "Epoch 1/1\n",
            "745/745 [==============================] - 7s 9ms/step - loss: 6.0887\n",
            "Epoch 1/1\n",
            "1401/1401 [==============================] - 12s 9ms/step - loss: 5.9464\n",
            "Epoch 1/1\n",
            "445/445 [==============================] - 4s 10ms/step - loss: 6.0286\n",
            "Epoch 1/1\n",
            "428/428 [==============================] - 1s 2ms/step - loss: 7.2945\n",
            "Epoch 1/1\n",
            "259/259 [==============================] - 3s 11ms/step - loss: 6.1336\n",
            "Epoch 1/1\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 6.4627\n",
            "Epoch 1/1\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 6.3262\n",
            "Epoch 1/1\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 6.1288\n",
            "Epoch 1/1\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 6.5569\n",
            "Epoch 1/1\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 6.2379\n",
            "Epoch 1/1\n",
            "13/13 [==============================] - 0s 34ms/step - loss: 6.3564\n",
            "Epoch 1/1\n",
            "45/45 [==============================] - 0s 2ms/step - loss: 8.0360\n",
            "Epoch 1/1\n",
            "6/6 [==============================] - 0s 66ms/step - loss: 6.6081\n",
            "Epoch 1/1\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 6.4313\n",
            "Epoch 1/1\n",
            "10/10 [==============================] - 0s 43ms/step - loss: 6.5306\n",
            "Epoch 1/1\n",
            "24/24 [==============================] - 0s 21ms/step - loss: 6.4657\n",
            "Epoch 1/1\n",
            "2/2 [==============================] - 0s 217ms/step - loss: 6.8849\n",
            "Epoch 1/1\n",
            "7/7 [==============================] - 0s 60ms/step - loss: 6.4123\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 1s 632ms/step - loss: 6.4506\n",
            "Epoch 1/1\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 6.1700\n",
            "Epoch 1/1\n",
            "2/2 [==============================] - 0s 223ms/step - loss: 6.2005\n",
            "Epoch 1/1\n",
            "2/2 [==============================] - 0s 210ms/step - loss: 5.7359\n",
            "---------TRUE EPOCH 4/3--------------\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 37s 6ms/step - loss: 5.6697\n",
            "Epoch 1/1\n",
            "2378/2378 [==============================] - 15s 6ms/step - loss: 5.5887\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 29s 5ms/step - loss: 5.5621\n",
            "Epoch 1/1\n",
            "5463/5463 [==============================] - 27s 5ms/step - loss: 5.3534\n",
            "Epoch 1/1\n",
            "5960/5960 [==============================] - 40s 7ms/step - loss: 5.7005\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 33s 5ms/step - loss: 5.5420\n",
            "Epoch 1/1\n",
            "4502/4502 [==============================] - 25s 5ms/step - loss: 5.4982\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 21s 4ms/step - loss: 5.6723\n",
            "Epoch 1/1\n",
            "1073/1073 [==============================] - 4s 4ms/step - loss: 5.3728\n",
            "Epoch 1/1\n",
            "3834/3834 [==============================] - 28s 7ms/step - loss: 5.8719\n",
            "Epoch 1/1\n",
            "2420/2420 [==============================] - 19s 8ms/step - loss: 5.7430\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 25s 4ms/step - loss: 5.5452\n",
            "Epoch 1/1\n",
            "4244/4244 [==============================] - 18s 4ms/step - loss: 5.4667\n",
            "Epoch 1/1\n",
            "1805/1805 [==============================] - 5s 3ms/step - loss: 6.9928\n",
            "Epoch 1/1\n",
            "745/745 [==============================] - 7s 9ms/step - loss: 6.0407\n",
            "Epoch 1/1\n",
            "1401/1401 [==============================] - 12s 9ms/step - loss: 5.9065\n",
            "Epoch 1/1\n",
            "445/445 [==============================] - 4s 10ms/step - loss: 5.9873\n",
            "Epoch 1/1\n",
            "428/428 [==============================] - 1s 2ms/step - loss: 7.2046\n",
            "Epoch 1/1\n",
            "259/259 [==============================] - 3s 11ms/step - loss: 6.0953\n",
            "Epoch 1/1\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 6.4213\n",
            "Epoch 1/1\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 6.2828\n",
            "Epoch 1/1\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 6.0883\n",
            "Epoch 1/1\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 6.4935\n",
            "Epoch 1/1\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 6.1915\n",
            "Epoch 1/1\n",
            "13/13 [==============================] - 0s 34ms/step - loss: 6.3042\n",
            "Epoch 1/1\n",
            "45/45 [==============================] - 0s 2ms/step - loss: 8.0117\n",
            "Epoch 1/1\n",
            "6/6 [==============================] - 0s 66ms/step - loss: 6.5617\n",
            "Epoch 1/1\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 6.3896\n",
            "Epoch 1/1\n",
            "10/10 [==============================] - 0s 44ms/step - loss: 6.4845\n",
            "Epoch 1/1\n",
            "24/24 [==============================] - 0s 21ms/step - loss: 6.4248\n",
            "Epoch 1/1\n",
            "2/2 [==============================] - 0s 218ms/step - loss: 6.8284\n",
            "Epoch 1/1\n",
            "7/7 [==============================] - 0s 60ms/step - loss: 6.3613\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 1s 628ms/step - loss: 6.3846\n",
            "Epoch 1/1\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 6.1266\n",
            "Epoch 1/1\n",
            "2/2 [==============================] - 0s 223ms/step - loss: 6.1682\n",
            "Epoch 1/1\n",
            "2/2 [==============================] - 0s 211ms/step - loss: 5.6993\n",
            "---------TRUE EPOCH 5/3--------------\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 37s 6ms/step - loss: 5.6523\n",
            "Epoch 1/1\n",
            "2378/2378 [==============================] - 15s 6ms/step - loss: 5.5697\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 29s 5ms/step - loss: 5.5527\n",
            "Epoch 1/1\n",
            "5463/5463 [==============================] - 27s 5ms/step - loss: 5.3425\n",
            "Epoch 1/1\n",
            "5960/5960 [==============================] - 40s 7ms/step - loss: 5.6726\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 33s 5ms/step - loss: 5.5241\n",
            "Epoch 1/1\n",
            "4502/4502 [==============================] - 25s 5ms/step - loss: 5.4823\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 21s 4ms/step - loss: 5.6579\n",
            "Epoch 1/1\n",
            "1073/1073 [==============================] - 4s 4ms/step - loss: 5.3852\n",
            "Epoch 1/1\n",
            "3834/3834 [==============================] - 28s 7ms/step - loss: 5.8174\n",
            "Epoch 1/1\n",
            "2420/2420 [==============================] - 19s 8ms/step - loss: 5.7112\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 25s 4ms/step - loss: 5.5281\n",
            "Epoch 1/1\n",
            "4244/4244 [==============================] - 18s 4ms/step - loss: 5.4500\n",
            "Epoch 1/1\n",
            "1805/1805 [==============================] - 5s 3ms/step - loss: 6.9469\n",
            "Epoch 1/1\n",
            "745/745 [==============================] - 7s 9ms/step - loss: 5.9957\n",
            "Epoch 1/1\n",
            "1401/1401 [==============================] - 12s 9ms/step - loss: 5.8692\n",
            "Epoch 1/1\n",
            "445/445 [==============================] - 4s 10ms/step - loss: 5.9534\n",
            "Epoch 1/1\n",
            "428/428 [==============================] - 1s 2ms/step - loss: 7.1350\n",
            "Epoch 1/1\n",
            "259/259 [==============================] - 3s 11ms/step - loss: 6.0666\n",
            "Epoch 1/1\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 6.3951\n",
            "Epoch 1/1\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 6.2533\n",
            "Epoch 1/1\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 6.0604\n",
            "Epoch 1/1\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 6.4613\n",
            "Epoch 1/1\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 6.1627\n",
            "Epoch 1/1\n",
            "13/13 [==============================] - 0s 34ms/step - loss: 6.2748\n",
            "Epoch 1/1\n",
            "45/45 [==============================] - 0s 2ms/step - loss: 7.9967\n",
            "Epoch 1/1\n",
            "6/6 [==============================] - 0s 65ms/step - loss: 6.5376\n",
            "Epoch 1/1\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 6.3639\n",
            "Epoch 1/1\n",
            "10/10 [==============================] - 0s 43ms/step - loss: 6.4558\n",
            "Epoch 1/1\n",
            "24/24 [==============================] - 0s 21ms/step - loss: 6.4003\n",
            "Epoch 1/1\n",
            "2/2 [==============================] - 0s 217ms/step - loss: 6.7979\n",
            "Epoch 1/1\n",
            "7/7 [==============================] - 0s 61ms/step - loss: 6.3361\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 1s 631ms/step - loss: 6.3451\n",
            "Epoch 1/1\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 6.0999\n",
            "Epoch 1/1\n",
            "2/2 [==============================] - 0s 226ms/step - loss: 6.1442\n",
            "Epoch 1/1\n",
            "2/2 [==============================] - 0s 210ms/step - loss: 5.6781\n",
            "---------TRUE EPOCH 6/3--------------\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 37s 6ms/step - loss: 5.6380\n",
            "Epoch 1/1\n",
            "2378/2378 [==============================] - 15s 6ms/step - loss: 5.5545\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 29s 5ms/step - loss: 5.5426\n",
            "Epoch 1/1\n",
            "5463/5463 [==============================] - 27s 5ms/step - loss: 5.3331\n",
            "Epoch 1/1\n",
            "5960/5960 [==============================] - 40s 7ms/step - loss: 5.6471\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 33s 5ms/step - loss: 5.5099\n",
            "Epoch 1/1\n",
            "4502/4502 [==============================] - 24s 5ms/step - loss: 5.4680\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 21s 4ms/step - loss: 5.6418\n",
            "Epoch 1/1\n",
            "1073/1073 [==============================] - 4s 4ms/step - loss: 5.3942\n",
            "Epoch 1/1\n",
            "3834/3834 [==============================] - 28s 7ms/step - loss: 5.7763\n",
            "Epoch 1/1\n",
            "2420/2420 [==============================] - 19s 8ms/step - loss: 5.6844\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 25s 4ms/step - loss: 5.5150\n",
            "Epoch 1/1\n",
            "4244/4244 [==============================] - 18s 4ms/step - loss: 5.4368\n",
            "Epoch 1/1\n",
            "1805/1805 [==============================] - 5s 3ms/step - loss: 6.9086\n",
            "Epoch 1/1\n",
            "745/745 [==============================] - 7s 9ms/step - loss: 5.9595\n",
            "Epoch 1/1\n",
            "1401/1401 [==============================] - 12s 9ms/step - loss: 5.8387\n",
            "Epoch 1/1\n",
            "445/445 [==============================] - 4s 10ms/step - loss: 5.9290\n",
            "Epoch 1/1\n",
            "428/428 [==============================] - 1s 2ms/step - loss: 7.0880\n",
            "Epoch 1/1\n",
            "259/259 [==============================] - 3s 11ms/step - loss: 6.0456\n",
            "Epoch 1/1\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 6.3771\n",
            "Epoch 1/1\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 6.2315\n",
            "Epoch 1/1\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 6.0401\n",
            "Epoch 1/1\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 6.4388\n",
            "Epoch 1/1\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 6.1423\n",
            "Epoch 1/1\n",
            "13/13 [==============================] - 0s 34ms/step - loss: 6.2537\n",
            "Epoch 1/1\n",
            "45/45 [==============================] - 0s 2ms/step - loss: 7.9894\n",
            "Epoch 1/1\n",
            "6/6 [==============================] - 0s 66ms/step - loss: 6.5192\n",
            "Epoch 1/1\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 6.3441\n",
            "Epoch 1/1\n",
            "10/10 [==============================] - 0s 44ms/step - loss: 6.4349\n",
            "Epoch 1/1\n",
            "24/24 [==============================] - 0s 21ms/step - loss: 6.3836\n",
            "Epoch 1/1\n",
            "2/2 [==============================] - 0s 217ms/step - loss: 6.7770\n",
            "Epoch 1/1\n",
            "7/7 [==============================] - 0s 60ms/step - loss: 6.3192\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 1s 631ms/step - loss: 6.3144\n",
            "Epoch 1/1\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 6.0781\n",
            "Epoch 1/1\n",
            "2/2 [==============================] - 0s 223ms/step - loss: 6.1293\n",
            "Epoch 1/1\n",
            "2/2 [==============================] - 0s 211ms/step - loss: 5.6582\n",
            "---------TRUE EPOCH 7/3--------------\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 37s 6ms/step - loss: 5.6230\n",
            "Epoch 1/1\n",
            "2378/2378 [==============================] - 15s 6ms/step - loss: 5.5406\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 29s 5ms/step - loss: 5.5338\n",
            "Epoch 1/1\n",
            "5463/5463 [==============================] - 27s 5ms/step - loss: 5.3249\n",
            "Epoch 1/1\n",
            "5960/5960 [==============================] - 40s 7ms/step - loss: 5.6244\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 33s 5ms/step - loss: 5.4967\n",
            "Epoch 1/1\n",
            "4502/4502 [==============================] - 25s 6ms/step - loss: 5.4551\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 22s 4ms/step - loss: 5.6270\n",
            "Epoch 1/1\n",
            "1073/1073 [==============================] - 4s 4ms/step - loss: 5.4021\n",
            "Epoch 1/1\n",
            "3834/3834 [==============================] - 28s 7ms/step - loss: 5.7417\n",
            "Epoch 1/1\n",
            "2420/2420 [==============================] - 19s 8ms/step - loss: 5.6611\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 25s 4ms/step - loss: 5.5039\n",
            "Epoch 1/1\n",
            "4244/4244 [==============================] - 18s 4ms/step - loss: 5.4264\n",
            "Epoch 1/1\n",
            "1805/1805 [==============================] - 5s 3ms/step - loss: 6.8749\n",
            "Epoch 1/1\n",
            "745/745 [==============================] - 7s 9ms/step - loss: 5.9288\n",
            "Epoch 1/1\n",
            "1401/1401 [==============================] - 12s 9ms/step - loss: 5.8121\n",
            "Epoch 1/1\n",
            "445/445 [==============================] - 4s 10ms/step - loss: 5.9074\n",
            "Epoch 1/1\n",
            "428/428 [==============================] - 1s 2ms/step - loss: 7.0522\n",
            "Epoch 1/1\n",
            "259/259 [==============================] - 3s 11ms/step - loss: 6.0273\n",
            "Epoch 1/1\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 6.3613\n",
            "Epoch 1/1\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 6.2126\n",
            "Epoch 1/1\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 6.0227\n",
            "Epoch 1/1\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 6.4212\n",
            "Epoch 1/1\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 6.1249\n",
            "Epoch 1/1\n",
            "13/13 [==============================] - 0s 34ms/step - loss: 6.2359\n",
            "Epoch 1/1\n",
            "45/45 [==============================] - 0s 2ms/step - loss: 7.9862\n",
            "Epoch 1/1\n",
            "6/6 [==============================] - 0s 66ms/step - loss: 6.5045\n",
            "Epoch 1/1\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 6.3283\n",
            "Epoch 1/1\n",
            "10/10 [==============================] - 0s 43ms/step - loss: 6.4183\n",
            "Epoch 1/1\n",
            "24/24 [==============================] - 0s 20ms/step - loss: 6.3690\n",
            "Epoch 1/1\n",
            "2/2 [==============================] - 0s 217ms/step - loss: 6.7583\n",
            "Epoch 1/1\n",
            "7/7 [==============================] - 0s 60ms/step - loss: 6.3045\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 1s 629ms/step - loss: 6.2833\n",
            "Epoch 1/1\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 6.0596\n",
            "Epoch 1/1\n",
            "2/2 [==============================] - 0s 224ms/step - loss: 6.1185\n",
            "Epoch 1/1\n",
            "2/2 [==============================] - 0s 211ms/step - loss: 5.6420\n",
            "---------TRUE EPOCH 8/3--------------\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 36s 6ms/step - loss: 5.6067\n",
            "Epoch 1/1\n",
            "2378/2378 [==============================] - 15s 6ms/step - loss: 5.5269\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 29s 5ms/step - loss: 5.5229\n",
            "Epoch 1/1\n",
            "5463/5463 [==============================] - 27s 5ms/step - loss: 5.3168\n",
            "Epoch 1/1\n",
            "5960/5960 [==============================] - 40s 7ms/step - loss: 5.6056\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 33s 5ms/step - loss: 5.4861\n",
            "Epoch 1/1\n",
            "4502/4502 [==============================] - 25s 5ms/step - loss: 5.4439\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 22s 4ms/step - loss: 5.6118\n",
            "Epoch 1/1\n",
            "1073/1073 [==============================] - 4s 4ms/step - loss: 5.4056\n",
            "Epoch 1/1\n",
            "3834/3834 [==============================] - 28s 7ms/step - loss: 5.7134\n",
            "Epoch 1/1\n",
            "2420/2420 [==============================] - 19s 8ms/step - loss: 5.6415\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 25s 4ms/step - loss: 5.4954\n",
            "Epoch 1/1\n",
            "4244/4244 [==============================] - 18s 4ms/step - loss: 5.4185\n",
            "Epoch 1/1\n",
            "1805/1805 [==============================] - 5s 3ms/step - loss: 6.8442\n",
            "Epoch 1/1\n",
            "745/745 [==============================] - 7s 9ms/step - loss: 5.9022\n",
            "Epoch 1/1\n",
            "1401/1401 [==============================] - 12s 9ms/step - loss: 5.7891\n",
            "Epoch 1/1\n",
            "445/445 [==============================] - 4s 10ms/step - loss: 5.8894\n",
            "Epoch 1/1\n",
            "428/428 [==============================] - 1s 2ms/step - loss: 7.0222\n",
            "Epoch 1/1\n",
            "259/259 [==============================] - 3s 11ms/step - loss: 6.0120\n",
            "Epoch 1/1\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 6.3490\n",
            "Epoch 1/1\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 6.1979\n",
            "Epoch 1/1\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 6.0088\n",
            "Epoch 1/1\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 6.4065\n",
            "Epoch 1/1\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 6.1107\n",
            "Epoch 1/1\n",
            "13/13 [==============================] - 0s 34ms/step - loss: 6.2216\n",
            "Epoch 1/1\n",
            "45/45 [==============================] - 0s 2ms/step - loss: 7.9824\n",
            "Epoch 1/1\n",
            "6/6 [==============================] - 0s 66ms/step - loss: 6.4920\n",
            "Epoch 1/1\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 6.3145\n",
            "Epoch 1/1\n",
            "10/10 [==============================] - 0s 43ms/step - loss: 6.4056\n",
            "Epoch 1/1\n",
            "24/24 [==============================] - 0s 21ms/step - loss: 6.3585\n",
            "Epoch 1/1\n",
            "2/2 [==============================] - 0s 217ms/step - loss: 6.7461\n",
            "Epoch 1/1\n",
            "7/7 [==============================] - 0s 60ms/step - loss: 6.2932\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 1s 629ms/step - loss: 6.2612\n",
            "Epoch 1/1\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 6.0434\n",
            "Epoch 1/1\n",
            "2/2 [==============================] - 0s 224ms/step - loss: 6.1063\n",
            "Epoch 1/1\n",
            "2/2 [==============================] - 0s 210ms/step - loss: 5.6270\n",
            "---------TRUE EPOCH 9/3--------------\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 37s 6ms/step - loss: 5.5937\n",
            "Epoch 1/1\n",
            "2378/2378 [==============================] - 15s 6ms/step - loss: 5.5150\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 29s 5ms/step - loss: 5.5134\n",
            "Epoch 1/1\n",
            "5463/5463 [==============================] - 27s 5ms/step - loss: 5.3099\n",
            "Epoch 1/1\n",
            "5960/5960 [==============================] - 40s 7ms/step - loss: 5.5882\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 33s 5ms/step - loss: 5.4759\n",
            "Epoch 1/1\n",
            "4502/4502 [==============================] - 25s 5ms/step - loss: 5.4339\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 22s 4ms/step - loss: 5.5958\n",
            "Epoch 1/1\n",
            "1073/1073 [==============================] - 4s 4ms/step - loss: 5.4060\n",
            "Epoch 1/1\n",
            "3834/3834 [==============================] - 28s 7ms/step - loss: 5.6900\n",
            "Epoch 1/1\n",
            "2420/2420 [==============================] - 19s 8ms/step - loss: 5.6248\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 25s 4ms/step - loss: 5.4878\n",
            "Epoch 1/1\n",
            "4244/4244 [==============================] - 18s 4ms/step - loss: 5.4122\n",
            "Epoch 1/1\n",
            "1805/1805 [==============================] - 5s 3ms/step - loss: 6.8155\n",
            "Epoch 1/1\n",
            "745/745 [==============================] - 7s 9ms/step - loss: 5.8791\n",
            "Epoch 1/1\n",
            "1401/1401 [==============================] - 12s 9ms/step - loss: 5.7689\n",
            "Epoch 1/1\n",
            "445/445 [==============================] - 4s 10ms/step - loss: 5.8730\n",
            "Epoch 1/1\n",
            "428/428 [==============================] - 1s 2ms/step - loss: 6.9949\n",
            "Epoch 1/1\n",
            "259/259 [==============================] - 3s 11ms/step - loss: 5.9981\n",
            "Epoch 1/1\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 6.3384\n",
            "Epoch 1/1\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 6.1848\n",
            "Epoch 1/1\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 5.9960\n",
            "Epoch 1/1\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 6.3937\n",
            "Epoch 1/1\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 6.0987\n",
            "Epoch 1/1\n",
            "13/13 [==============================] - 0s 33ms/step - loss: 6.2090\n",
            "Epoch 1/1\n",
            "45/45 [==============================] - 0s 2ms/step - loss: 7.9811\n",
            "Epoch 1/1\n",
            "6/6 [==============================] - 0s 65ms/step - loss: 6.4820\n",
            "Epoch 1/1\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 6.3035\n",
            "Epoch 1/1\n",
            "10/10 [==============================] - 0s 43ms/step - loss: 6.3938\n",
            "Epoch 1/1\n",
            "24/24 [==============================] - 0s 20ms/step - loss: 6.3487\n",
            "Epoch 1/1\n",
            "2/2 [==============================] - 0s 215ms/step - loss: 6.7354\n",
            "Epoch 1/1\n",
            "7/7 [==============================] - 0s 59ms/step - loss: 6.2838\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 1s 632ms/step - loss: 6.2405\n",
            "Epoch 1/1\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 6.0294\n",
            "Epoch 1/1\n",
            "2/2 [==============================] - 0s 222ms/step - loss: 6.0948\n",
            "Epoch 1/1\n",
            "2/2 [==============================] - 0s 209ms/step - loss: 5.6133\n",
            "---------TRUE EPOCH 10/3--------------\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 36s 6ms/step - loss: 5.5806\n",
            "Epoch 1/1\n",
            "2378/2378 [==============================] - 15s 6ms/step - loss: 5.5034\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 29s 5ms/step - loss: 5.5045\n",
            "Epoch 1/1\n",
            "5463/5463 [==============================] - 27s 5ms/step - loss: 5.3032\n",
            "Epoch 1/1\n",
            "5960/5960 [==============================] - 40s 7ms/step - loss: 5.5735\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 33s 5ms/step - loss: 5.4671\n",
            "Epoch 1/1\n",
            "4502/4502 [==============================] - 25s 5ms/step - loss: 5.4248\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 22s 4ms/step - loss: 5.5827\n",
            "Epoch 1/1\n",
            "1073/1073 [==============================] - 4s 4ms/step - loss: 5.4070\n",
            "Epoch 1/1\n",
            "3834/3834 [==============================] - 28s 7ms/step - loss: 5.6688\n",
            "Epoch 1/1\n",
            "2420/2420 [==============================] - 19s 8ms/step - loss: 5.6096\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 25s 4ms/step - loss: 5.4811\n",
            "Epoch 1/1\n",
            "4244/4244 [==============================] - 18s 4ms/step - loss: 5.4070\n",
            "Epoch 1/1\n",
            "1805/1805 [==============================] - 5s 3ms/step - loss: 6.7903\n",
            "Epoch 1/1\n",
            "745/745 [==============================] - 7s 9ms/step - loss: 5.8570\n",
            "Epoch 1/1\n",
            "1401/1401 [==============================] - 12s 9ms/step - loss: 5.7497\n",
            "Epoch 1/1\n",
            "445/445 [==============================] - 4s 10ms/step - loss: 5.8570\n",
            "Epoch 1/1\n",
            "428/428 [==============================] - 1s 2ms/step - loss: 6.9698\n",
            "Epoch 1/1\n",
            "259/259 [==============================] - 3s 11ms/step - loss: 5.9845\n",
            "Epoch 1/1\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 6.3273\n",
            "Epoch 1/1\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 6.1716\n",
            "Epoch 1/1\n",
            "133/133 [==============================] - 2s 12ms/step - loss: 5.9834\n",
            "Epoch 1/1\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 6.3797\n",
            "Epoch 1/1\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 6.0856\n",
            "Epoch 1/1\n",
            "13/13 [==============================] - 0s 34ms/step - loss: 6.1963\n",
            "Epoch 1/1\n",
            "45/45 [==============================] - 0s 2ms/step - loss: 7.9778\n",
            "Epoch 1/1\n",
            "6/6 [==============================] - 0s 66ms/step - loss: 6.4708\n",
            "Epoch 1/1\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 6.2910\n",
            "Epoch 1/1\n",
            "10/10 [==============================] - 0s 44ms/step - loss: 6.3822\n",
            "Epoch 1/1\n",
            "24/24 [==============================] - 0s 21ms/step - loss: 6.3388\n",
            "Epoch 1/1\n",
            "2/2 [==============================] - 0s 217ms/step - loss: 6.7246\n",
            "Epoch 1/1\n",
            "7/7 [==============================] - 0s 60ms/step - loss: 6.2729\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 1s 632ms/step - loss: 6.2188\n",
            "Epoch 1/1\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 6.0147\n",
            "Epoch 1/1\n",
            "2/2 [==============================] - 0s 223ms/step - loss: 6.0828\n",
            "Epoch 1/1\n",
            "2/2 [==============================] - 0s 211ms/step - loss: 5.5976\n",
            "True epoch 1: [5.76201057434082]\n",
            "True epoch 2: [5.760293960571289]\n",
            "True epoch 3: [5.735911846160889]\n",
            "True epoch 4: [5.699262619018555]\n",
            "True epoch 5: [5.678117752075195]\n",
            "True epoch 6: [5.658195972442627]\n",
            "True epoch 7: [5.642022609710693]\n",
            "True epoch 8: [5.627007961273193]\n",
            "True epoch 9: [5.613335132598877]\n",
            "True epoch 10: [5.597573757171631]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TcnTje7akW5M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model.save_weights('model_v1-1_13_epochs.h5')\n",
        "# files.download('model_v1-1_13_epochs.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zmgZreHFsMuj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sample"
      ]
    },
    {
      "metadata": {
        "id": "UqrIqaNrsLbW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def inference_model():\n",
        "  x0 = Input(shape=(None, word_voc_size))\n",
        "  a0 = Input(shape=(n_a,), name='a0')\n",
        "  c0 = Input(shape=(n_a,), name='c0')\n",
        "  x = x0\n",
        "  a = a0\n",
        "  c = c0\n",
        "  \n",
        "  _, a, c = lstm(x, initial_state=[a,c])\n",
        "  prob = densor(a)\n",
        "  \n",
        "  return Model(inputs=[x0,a0,c0], outputs=[prob,a,c])\n",
        "\n",
        "inf_model = inference_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UIQcYqqtuRJF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2672156b-9c11-431c-e2a7-4f8669445bb6"
      },
      "cell_type": "code",
      "source": [
        "def generate_dajare(max_len, sample_method='max'):\n",
        "  words = list()\n",
        "  word = None\n",
        "  x = np.zeros((1, 1, word_voc_size)) # 1 sample, length 1\n",
        "  a = np.zeros((1, n_a,))\n",
        "  c = np.zeros((1, n_a,))\n",
        "  while len(words) <= max_len and word != '\\n':\n",
        "    prob, a, c = inf_model.predict([x,a,c])\n",
        "    # assert prob.shape == (1, word_voc_size)\n",
        "    if sample_method == 'max':\n",
        "      i = np.argmax(prob) # automatically flattened\n",
        "    else:\n",
        "      i = np.random.choice(a = word_voc_size, p = prob.ravel()) # select element from array a by probability 1d array p\n",
        "    word = i2w[i]\n",
        "    words.append(word)\n",
        "    x = np.array( [to_one_hot_arr([i], word_voc_size)] )\n",
        "    # assert x.shape == (1,1, word_voc_size)\n",
        "  \n",
        "  return words"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ",!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KyIJZ9yR82wH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "words = generate_dajare(30, 'random')\n",
        "print(''.join(words))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}